{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICg1mDgHczGM"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"\"\n",
        "INPUT = \"\"\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "def convert_result(result):\n",
        "  requirements = \"\"\n",
        "  terms = \"\"\n",
        "  notes = \"\"\n",
        "  for span in result:\n",
        "    label = span[\"entity_group\"]\n",
        "    word = span[\"word\"]\n",
        "    if label == \"requirements\":\n",
        "      requirements += word + \" \"\n",
        "    elif label == \"terms\":\n",
        "      terms += word + \" \"\n",
        "    elif label == \"notes\":\n",
        "      notes += word + \" \"\n",
        "  return requirements, terms, notes\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH).eval().to(device)\n",
        "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
        "raw_result = pipe(INPUT)\n",
        "result = convert_result(result)"
      ]
    }
  ]
}